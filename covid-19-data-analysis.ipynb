{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/williamkaiser/covid-19-data-analysis?scriptVersionId=89705836\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown","outputs":[],"execution_count":0},{"cell_type":"markdown","source":"# Analysis of Papers Written About COVID 19 Based on Impact and Text Content\nThis started off really organized, but now it is not. I need things to run fast!!!! Fuck!","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport nltk\nimport os\n\n# Monitors the Status of the Project\nfrom tqdm import tqdm\ntqdm.pandas()\n\n# The Dark Arts\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\")","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:11:47.562078Z","iopub.execute_input":"2022-03-09T15:11:47.562378Z","iopub.status.idle":"2022-03-09T15:11:48.820834Z","shell.execute_reply.started":"2022-03-09T15:11:47.562297Z","shell.execute_reply":"2022-03-09T15:11:48.820114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# SYSTEM VARIABLES\nFRAC_DATASET = 1 # Runs analysis on only one percent of the dataset\nINC_THRESH = 0.25 # At least 25 % of the impact data has to be present\n\ndata = pd.read_csv(\"/kaggle/input/CORD-19-research-challenge/metadata.csv\", dtype=\"string\")\n\ndata.head()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-03-09T15:11:48.823484Z","iopub.execute_input":"2022-03-09T15:11:48.823767Z","iopub.status.idle":"2022-03-09T15:12:20.147392Z","shell.execute_reply.started":"2022-03-09T15:11:48.823705Z","shell.execute_reply":"2022-03-09T15:12:20.14664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removes the non-essential data\ndata = data.sample(frac=FRAC_DATASET)\n\nN_ROWS = len(data.index)\nN_COLS = len(data.columns)\n\ndata.dropna(subset=['doi', 'title', 'abstract', 'url', 'cord_uid'], inplace=True)\n\nprint(f\"number of columns:       {N_COLS:10d}\")\nprint(f\"original number of rows: {N_ROWS:10d}\")\nprint(f\"new number of rows:      {len(data.index):10d}\")\nprint(f\"Percent Removed:         {(1 - len(data.index) / N_ROWS) * 100:9.2f}%\")  ","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:12:20.148703Z","iopub.execute_input":"2022-03-09T15:12:20.148951Z","iopub.status.idle":"2022-03-09T15:12:23.831843Z","shell.execute_reply.started":"2022-03-09T15:12:20.148919Z","shell.execute_reply":"2022-03-09T15:12:23.831077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Exploratory Data Analysis\nHere we learn more about the dataset and the papers that it contains. This will be used to guide later analysis.","metadata":{}},{"cell_type":"code","source":"from datetime import datetime\nimport matplotlib.pyplot as plt\n\ndef get_date(date):\n    if date.find(\"-\") != -1:\n        return datetime.strptime(date, \"%Y-%m-%d\")\n    else:\n        return datetime.strptime(date, \"%Y\")\n\n# Time Created Histogram\npublish_times = data['publish_time'].dropna().apply(get_date)\npublish_times = publish_times[publish_times > datetime.strptime(\"2000-1-1\", \"%Y-%m-%d\")]\nplt.hist(publish_times, bins=20)\nplt.title(\"When Papers on COVID-19 Were Published\")\nplt.xlabel(\"Date Published\")\nplt.ylabel(\"Number of Papers\")\nplt.savefig('paper_ages.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:12:23.833913Z","iopub.execute_input":"2022-03-09T15:12:23.834256Z","iopub.status.idle":"2022-03-09T15:12:28.626493Z","shell.execute_reply.started":"2022-03-09T15:12:23.834217Z","shell.execute_reply":"2022-03-09T15:12:28.625628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Comparing the length of abstracts to the length of titles to determine compute time\nabstract_char_count = data['abstract'].apply(len).sum()\ntitle_char_count = data['title'].apply(len).sum()\n\nprint(f\"abstracts are {abstract_char_count / title_char_count:10.2f}x larger\")","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:12:28.627885Z","iopub.execute_input":"2022-03-09T15:12:28.628278Z","iopub.status.idle":"2022-03-09T15:12:29.082955Z","shell.execute_reply.started":"2022-03-09T15:12:28.628241Z","shell.execute_reply":"2022-03-09T15:12:29.082186Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Impact Data and Analysis\nHere, I am trying to learn more about what kind of impact these papers have and their distributions. Pending these results, I will consider splitting my analysis between high and low impact papers. Additionally, I need to linearize this data regardless.","metadata":{}},{"cell_type":"code","source":"# Gets a list of API Calls to make (will be joined back up later)\ndef get_request(doi): \n    return f\"https://api.altmetric.com/v1/doi/{doi}\"\n    \ncalls = data['doi'].apply(lambda doi: f\"https://api.altmetric.com/v1/doi/{doi}\").to_frame()\ncalls['cord_uid'] = data['cord_uid']  \ncalls.to_csv('altmetric_calls.csv', index=False)\ndel calls","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:12:29.08414Z","iopub.execute_input":"2022-03-09T15:12:29.084861Z","iopub.status.idle":"2022-03-09T15:12:31.039157Z","shell.execute_reply.started":"2022-03-09T15:12:29.084822Z","shell.execute_reply":"2022-03-09T15:12:31.038401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"INC_THRESH = 0.5\n# Reads the impact data generated from alt metric to a file\nimpact = pd.read_csv('/kaggle/input/impact-data/impact_upload.csv')\n\n# Displays the Percentage of Null Values Per Column\nnull_values = impact.isna().sum().sort_values()\nprint(\"Where the most \")\nnull_values[null_values < INC_THRESH * len(null_values)]\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:12:31.040561Z","iopub.execute_input":"2022-03-09T15:12:31.040824Z","iopub.status.idle":"2022-03-09T15:12:31.463078Z","shell.execute_reply.started":"2022-03-09T15:12:31.040789Z","shell.execute_reply":"2022-03-09T15:12:31.462361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Drops the DOI from impact (only other overlapping column)\nimpact.drop(['doi'], axis=1, inplace=True)\n\n# Joins the two data frames together\ndata = data.set_index('cord_uid').join(impact.set_index('cord_uid'))\n\n# Removes NAs for any of the categories in place\ndata.dropna(subset=['score', 'readers_count', 'posts', 'accounts'], inplace=True) # Gets rid of everything without a full text","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:12:31.464385Z","iopub.execute_input":"2022-03-09T15:12:31.464796Z","iopub.status.idle":"2022-03-09T15:12:35.399246Z","shell.execute_reply.started":"2022-03-09T15:12:31.464758Z","shell.execute_reply":"2022-03-09T15:12:35.398519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"##### Good paper impact data:\n- score\n- readers_count\n- posts\n- accounts\n","metadata":{}},{"cell_type":"markdown","source":"# Textual Analysis\n1. Remove non-essential words (what, is, a)\n2. Turn each remaining word into its root (walking -> walk)\n3. Unsupervised learning on each title (going to expand this to abstracts in the future, but it took too long for testing)\n4. Apply the machine learning model to turn each sentence to a list of 100 numbers \n\nNote: a shift to the spacy bio parser will have to take place to actually read this information","metadata":{}},{"cell_type":"code","source":"# Language detection\n\n# Note: reading another paper that did textual analysis on this, they removed all of the non-english papers.\n\n# Only a very small percent of these papers were non-english, but I thought it would be a good idea\n!pip install langdetect\nfrom langdetect import detect\nfrom langdetect import DetectorFactory\n\n# Set the seed\nDetectorFactory.seed = 1231123123\n\nprint(\"Starting language detection\")\n\ndef get_language(row):\n    \"\"\"Gets the languages of each paper using fancy machine-learning\"\"\"\n    try:\n        return detect(' '.join(row[:min(len(row), 0)]))\n    except:\n        return 'en'\n    \ndata['lang'] = data['abstract'].progress_apply(get_language)\n\nprint(f\"Length Before {len(data.index):6d}\")\n\n\ndata = data[data['lang'] == 'en']\nprint(f\"Length After  {len(data.index):6d}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:12:35.400543Z","iopub.execute_input":"2022-03-09T15:12:35.400809Z","iopub.status.idle":"2022-03-09T15:12:54.5962Z","shell.execute_reply.started":"2022-03-09T15:12:35.400775Z","shell.execute_reply":"2022-03-09T15:12:54.595416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#NLP \nnull = !pip install -U spacy[cuda]\nnull = !pip install scispacy\nnull = !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_lg-0.4.0.tar.gz\n#null = !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.4.0/en_core_sci_sm-0.4.0.tar.gz\n\ndel null # Just mutes the text\n\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:12:54.59961Z","iopub.execute_input":"2022-03-09T15:12:54.599833Z","iopub.status.idle":"2022-03-09T15:14:26.342124Z","shell.execute_reply.started":"2022-03-09T15:12:54.599804Z","shell.execute_reply":"2022-03-09T15:14:26.338084Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = data.iloc[:80000,:] # Goes down to 60,000 entries (I am super worried about ram wtf!)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.345539Z","iopub.status.idle":"2022-03-09T15:14:26.347779Z","shell.execute_reply.started":"2022-03-09T15:14:26.347424Z","shell.execute_reply":"2022-03-09T15:14:26.347455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import string\n\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\nstopwords[:10]","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.352113Z","iopub.status.idle":"2022-03-09T15:14:26.354224Z","shell.execute_reply.started":"2022-03-09T15:14:26.353973Z","shell.execute_reply":"2022-03-09T15:14:26.354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"custom_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI'\n]\n\nfor w in custom_stop_words:\n    if w not in stopwords:\n        stopwords.append(w)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.357955Z","iopub.status.idle":"2022-03-09T15:14:26.358586Z","shell.execute_reply.started":"2022-03-09T15:14:26.358333Z","shell.execute_reply":"2022-03-09T15:14:26.358358Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Rows Before: {len(data.index):20d}\")\ndata.dropna(subset=['pdf_json_files'], inplace=True)\nprint(f\"Rows Before: {len(data.index):20d}\")\n\nimport json\nimport multiprocessing as mp\n\nroot_path = '/kaggle/input/CORD-19-research-challenge/'\n\ndef get_body_text(row):\n    try:\n        body_text = []\n        \n        with open(root_path + row.split('; ')[0], 'r') as f:\n            content = json.load(f)\n        body_text = []\n        \n        for entry in content['body_text']:\n            body_text.append(entry['text'])\n\n        body_text = '\\n'.join(body_text)\n        return body_text # Tokenizes the body text in one big go (hopefully faster!!)\n    except:\n        print('failed')\n        return None\n\ndef get_numb_files(row): \n    return len(row.split('; '))\n\nprint(f\"Avg Number of Files: {data['pdf_json_files'].progress_apply(get_numb_files).mean():14.2f}\")\n\nfull_text = data['pdf_json_files'].progress_apply(get_body_text)\n# full_text = data['abstract'] # Fuck naming convetions\n\ndata.to_csv('body_extract.csv', index=False)\n\nprint(f\"Rows After: {len(data.index):10d}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.359865Z","iopub.status.idle":"2022-03-09T15:14:26.36427Z","shell.execute_reply.started":"2022-03-09T15:14:26.363973Z","shell.execute_reply":"2022-03-09T15:14:26.364Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import gc\ngc.collect() # Does a little Garbage Collection\n\nimport scispacy\n\nspacy.prefer_gpu(gpu_id=0) # Does not actually bottleneck as GPU performance is limited\nparser = spacy.load(\"en_core_sci_lg\")\nparser.max_length = 7000000\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)    \n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens\n\ndata['processed_text'] = full_text.progress_apply(spacy_tokenizer)\ndel full_text\ndel parser","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.365559Z","iopub.status.idle":"2022-03-09T15:14:26.366213Z","shell.execute_reply.started":"2022-03-09T15:14:26.36598Z","shell.execute_reply":"2022-03-09T15:14:26.366007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Makes a histogram of paper length\nfrom math import log10\n\nlog_function = log10\n\npaper_length = data['processed_text'].progress_apply(lamda x: log_function(x + 1))\n\nplt.hist(paper_length, bins=50)\nplt.title(\"Log-Weighted Distribution of Paper Length\")\nplt.xlabel(\"Log Character Count\")\nplt.ylabel(\"Number of Papers\")\nplt.savefig(\"PaperLength.png\")\nplt.show()\n\ndel paper_length","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:26:49.577628Z","iopub.execute_input":"2022-03-09T15:26:49.57829Z","iopub.status.idle":"2022-03-09T15:26:49.656812Z","shell.execute_reply.started":"2022-03-09T15:26:49.578192Z","shell.execute_reply":"2022-03-09T15:26:49.655847Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print('text after processing')\nfor i in range(3):\n    print(f\"        {i}. {data.loc['processed_text', i][:100]}\")","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.367426Z","iopub.status.idle":"2022-03-09T15:14:26.368266Z","shell.execute_reply.started":"2022-03-09T15:14:26.367967Z","shell.execute_reply":"2022-03-09T15:14:26.367998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\ndef vectorize(text, maxx_features):\n    \n    vectorizer = TfidfVectorizer(max_features=maxx_features)\n    X = vectorizer.fit_transform(text)\n    return X\n\ntext = data['processed_text'].values\nmax_features = 2**12\n\nX = vectorize(text, max_features)\n\nimport pickle\npickle.dump(X, open('X-first-pickle.p', 'wb'))\n\ndel test","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.374701Z","iopub.status.idle":"2022-03-09T15:14:26.375372Z","shell.execute_reply.started":"2022-03-09T15:14:26.375107Z","shell.execute_reply":"2022-03-09T15:14:26.375131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Mini EDA of Impact\nTrying to linearize the distribution\n\nI am going to take the log of this","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport math\n\n# Histograms\nfor plot_type in ['score', 'readers_count', 'posts', 'accounts']:\n    print(plot_type)\n    # Logs eveythings\n    data[plot_type + \"_linearized\"] = data[plot_type].apply(lambda row: math.log10(row + 1))\n    \n    plt.hist(data[plot_type])\n    plt.title(f\"Impact Score of {plot_type}\")\n    plt.ylabel(\"# of Papers\")\n    plt.xlabel(\"Impact Score\")\n    plt.savefig(plot_type + \".png\")\n    plt.show()\n    \n    plt.hist(data[plot_type + \"_linearized\"])\n    plt.title(f\"Skew-Adjusted Impact Score of {plot_type}\")\n    plt.ylabel(\"# of Papers\")\n    plt.xlabel(\"Log10 of Impact Score\")\n    plt.savefig(plot_type + \"_linearized.png\")\n    plt.show()\n    \ndata['score'].max()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.37659Z","iopub.status.idle":"2022-03-09T15:14:26.377248Z","shell.execute_reply.started":"2022-03-09T15:14:26.376998Z","shell.execute_reply":"2022-03-09T15:14:26.377021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Score Histogram...\n\n# Top 1 percentile analysis\ntop_1 = data['score'].quantile(q=.99, interpolation='linear')\n\ntop_1_percent_score = data['score'][data['score'] > top_1]\n\nplt.hist(top_1_percent_score)\nplt.ylabel(\"# of Papers\")\nplt.xlabel('Impact Score')\nplt.title(\"Impact of Top 1% of Papers\")\nplt.savefig('top_1_score.png')\nplt.show()\n\n# Bottom 99 percentile analysis\nbottom_99_percent_score = data['score'][data['score'] < top_1]\n\nplt.hist(bottom_99_percent_score)\nplt.ylabel(\"# of Papers\")\nplt.xlabel('Impact Score')\nplt.title(\"Impact of Bottom 99% of Papers\")\nplt.savefig('bottom_99_score.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.378498Z","iopub.status.idle":"2022-03-09T15:14:26.379295Z","shell.execute_reply.started":"2022-03-09T15:14:26.379013Z","shell.execute_reply":"2022-03-09T15:14:26.379044Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Linearizing the Score Data\nfrom math import log10\n\ntop_1_lin = top_1_percent_score.apply(lambda x: log10(x + 1))\n\nplt.hist(top_1_lin)\nplt.title(\"Skew-Adjusted Impact of Top 1% of Papers\")\nplt.ylabel(\"# of Papers\")\nplt.xlabel(\"Log Impact\")\nplt.show()\n\nbottom_99_lin = bottom_99_percent_score.apply(lambda x: log10(x + 1))\n\nplt.hist(bottom_99_lin)\nplt.title(\"Skew-Adjusted Impact of Bottom 99% of Papers\")\nplt.ylabel(\"# of Papers\")\nplt.xlabel(\"Log Impact\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.380512Z","iopub.status.idle":"2022-03-09T15:14:26.380918Z","shell.execute_reply.started":"2022-03-09T15:14:26.380646Z","shell.execute_reply":"2022-03-09T15:14:26.380664Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Clustering the Dataset \n1. Uses Principle Component Analysis (PCA) to turn 100 columns of data into 3 (enough to be plotted on a 3D dot plot).\n2. Uses K-nearest neighbors to group papers based on the closest Euclidan distance\n3. 2D Graph of the data\n4. 3D graph of the data","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\npca = PCA(n_components=0.8, random_state=42) # I would like this to be 95% ideally.\nX_reduced = pca.fit_transform(X.toarray())\nX.shape\nX_reduced.shape","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.382502Z","iopub.status.idle":"2022-03-09T15:14:26.383517Z","shell.execute_reply.started":"2022-03-09T15:14:26.383276Z","shell.execute_reply":"2022-03-09T15:14:26.383302Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepares the Impact Data Using Linearization\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.384443Z","iopub.status.idle":"2022-03-09T15:14:26.384955Z","shell.execute_reply.started":"2022-03-09T15:14:26.384741Z","shell.execute_reply":"2022-03-09T15:14:26.384764Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cluster import MiniBatchKMeans\nfrom sklearn.cluster import KMeans\n\nfrom sklearn import metrics\nfrom scipy.spatial.distance import cdist\n\n# run kmeans with many different k\ndistortions = []\nK = [i for i in range(2, 30, 4)] # Does clusters in steps of 4 to save time...\nfor n_clusters in tqdm(K,total=len(K)):\n    kmeans = KMeans(n_clusters=n_clusters, random_state=42).fit(X_reduced)\n    # k_means.fit(X_reduced)\n    distortions.append(sum(np.min(cdist(X_reduced, kmeans.cluster_centers_, 'euclidean'), axis=1)) / X.shape[0])","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.386249Z","iopub.status.idle":"2022-03-09T15:14:26.387131Z","shell.execute_reply.started":"2022-03-09T15:14:26.386887Z","shell.execute_reply":"2022-03-09T15:14:26.38691Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_line = [K[0], K[-1]]\nY_line = [distortions[0], distortions[-1]]\n\n# Plot the elbow\nplt.plot(K, distortions, 'b-')\nplt.plot(X_line, Y_line, 'r')\nplt.xlabel('k')\nplt.ylabel('Distortion')\nplt.title('The Elbow Method showing the optimal k')\nplt.savefig('ElbowMethod.png')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.388363Z","iopub.status.idle":"2022-03-09T15:14:26.388847Z","shell.execute_reply.started":"2022-03-09T15:14:26.388576Z","shell.execute_reply":"2022-03-09T15:14:26.3886Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_fasdfs = X.copy()\nprint(X_reduced[:3,:3])\nmagnitudes = [np.linalg.norm(red) for red in X_reduced]\nprint(magnitudes[:3])\nplt.hist(magnitudes[:min(len(magnitudes), 1000)], bins=30)\nplt.savefig(\"reduced_weight.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.390208Z","iopub.status.idle":"2022-03-09T15:14:26.390482Z","shell.execute_reply.started":"2022-03-09T15:14:26.390333Z","shell.execute_reply":"2022-03-09T15:14:26.390352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"k = 18\nkmeans = KMeans(n_clusters=k, random_state=42)\ndata['cluster'] = kmeans.fit_predict(X_reduced)\nprint(kmeans.score(X_reduced))","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.392545Z","iopub.status.idle":"2022-03-09T15:14:26.393134Z","shell.execute_reply.started":"2022-03-09T15:14:26.392903Z","shell.execute_reply":"2022-03-09T15:14:26.392926Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.manifold import TSNE\n\n# TODO: turn up the perplexity for the actual run.\n\n# The perplexity should be 50 for 200,000, but I am going to use 45 because I am dumb.,..\ntsne = TSNE(verbose=1, perplexity=30, n_iter=1000)  # Changed perplexity from 100 to 50 per FAQ\nX_embedded = tsne.fit_transform(X)\n\nprint(X.shape)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.39426Z","iopub.status.idle":"2022-03-09T15:14:26.394849Z","shell.execute_reply.started":"2022-03-09T15:14:26.394611Z","shell.execute_reply":"2022-03-09T15:14:26.394635Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\n\ndata.to_pickle('data_final.p')\n\npickle.dump(X, open('x.p', 'wb'))\npickle.dump(X_reduced, open('X_reduced.p', 'wb'))","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.395975Z","iopub.status.idle":"2022-03-09T15:14:26.396551Z","shell.execute_reply.started":"2022-03-09T15:14:26.396329Z","shell.execute_reply":"2022-03-09T15:14:26.396352Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clustering the Top 1 % of Data","metadata":{}},{"cell_type":"code","source":"%matplotlib inline\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\n# sns settings\nsns.set(rc={'figure.figsize':(13,9)})\n\n# colors\npalette = sns.hls_palette(k, l=.4, s=.9)\n\n# plot\nsns.scatterplot(X_embedded[:,0], X_embedded[:,1], hue=data['cluster'], legend='full', palette=palette)\nplt.title('t-SNE with Kmeans Labels')\nplt.savefig(\"improved_cluster_tsne.png\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.397681Z","iopub.status.idle":"2022-03-09T15:14:26.398278Z","shell.execute_reply.started":"2022-03-09T15:14:26.398048Z","shell.execute_reply":"2022-03-09T15:14:26.398072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Vectorization\nWhere the common vectors for each cluster are found","metadata":{}},{"cell_type":"code","source":"from sklearn.decomposition import LatentDirichletAllocation\nfrom sklearn.feature_extraction.text import CountVectorizer\nvectorizers = []\n    \nfor ii in range(0, k):\n    # Creating a vectorizer\n    vectorizers.append(CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}'))","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.399619Z","iopub.status.idle":"2022-03-09T15:14:26.400228Z","shell.execute_reply.started":"2022-03-09T15:14:26.399984Z","shell.execute_reply":"2022-03-09T15:14:26.400008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorized_data = []\n\nfor current_cluster, cvec in enumerate(vectorizers):\n    try:\n        vectorized_data.append(cvec.fit_transform(data.loc[data['cluster'] == current_cluster, 'processed_text']))\n    except Exception as e:\n        print(\"Not enough instances in cluster: \" + str(current_cluster))\n        vectorized_data.append(None)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.401363Z","iopub.status.idle":"2022-03-09T15:14:26.401955Z","shell.execute_reply.started":"2022-03-09T15:14:26.401714Z","shell.execute_reply":"2022-03-09T15:14:26.40175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# number of topics per cluster\nNUM_TOPICS_PER_CLUSTER = 20\n\n\nlda_models = []\n\nfor ii in range(0, k):\n    # Latent Dirichlet Allocation Model\n    lda = LatentDirichletAllocation(n_components=NUM_TOPICS_PER_CLUSTER, max_iter=10, learning_method='online',verbose=False, random_state=42)\n    lda_models.append(lda)\n    \nlda_models[0]","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.40309Z","iopub.status.idle":"2022-03-09T15:14:26.403675Z","shell.execute_reply.started":"2022-03-09T15:14:26.403448Z","shell.execute_reply":"2022-03-09T15:14:26.403472Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clusters_lda_data = []\n\nfor current_cluster, lda in enumerate(tqdm(lda_models, total=len(lda_models))):   \n    if vectorized_data[current_cluster] != None:\n        clusters_lda_data.append((lda.fit_transform(vectorized_data[current_cluster])))","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.404827Z","iopub.status.idle":"2022-03-09T15:14:26.405411Z","shell.execute_reply.started":"2022-03-09T15:14:26.405187Z","shell.execute_reply":"2022-03-09T15:14:26.405212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Functions for printing keywords for each topic\ndef selected_topics(model, vectorizer, top_n=3):\n    current_words = []\n    keywords = []\n    \n    for idx, topic in enumerate(model.components_):\n        words = [(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]\n        for word in words:\n            if word[0] not in current_words:\n                keywords.append(word)\n                current_words.append(word[0])\n                \n    keywords.sort(key = lambda x: x[1])  \n    keywords.reverse()\n    return_values = []\n    for ii in keywords:\n        return_values.append(ii[0])\n    return return_values","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.406566Z","iopub.status.idle":"2022-03-09T15:14:26.407159Z","shell.execute_reply.started":"2022-03-09T15:14:26.406925Z","shell.execute_reply":"2022-03-09T15:14:26.406948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_keywords = []\nfor current_vectorizer, lda in enumerate(tqdm(lda_models)):\n    if vectorized_data[current_vectorizer] != None:\n        all_keywords.append(selected_topics(lda, vectorizers[current_vectorizer]))","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.408402Z","iopub.status.idle":"2022-03-09T15:14:26.409052Z","shell.execute_reply.started":"2022-03-09T15:14:26.408829Z","shell.execute_reply":"2022-03-09T15:14:26.408852Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f=open('topics.txt','w')\n\ncount = 0\n\nfor ii in all_keywords:\n\n    if vectorized_data[count] != None:\n        f.write(', '.join(ii) + \"\\n\")\n    else:\n        f.write(\"Not enough instances to be determined. \\n\")\n        f.write(', '.join(ii) + \"\\n\")\n    count += 1\n\nf.close()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.410208Z","iopub.status.idle":"2022-03-09T15:14:26.410796Z","shell.execute_reply.started":"2022-03-09T15:14:26.410557Z","shell.execute_reply":"2022-03-09T15:14:26.41058Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(all_keywords[:2][:5])\n\nmassive_all_keywords = []\n\nfor keywords in all_keywords:\n    massive_all_keywords += keywords\n\nkeyword_count = {}\nfor keyword in massive_all_keywords:\n    if keyword in keyword_count:\n        keyword_count[keyword] += 1\n    else:\n        keyword_count[keyword] = 1\nkeyword_count = dict(sorted(keyword_count.items(), key=lambda item: item[1], reverse=True))\n\nprint(f\"Number of Words in the Categories {len(massive_all_keywords):10d}\")\n\nn_bars = min(10, len(keyword_count.keys()))\n\nplt.bar([i for i in range(n_bars)], list(keyword_count.values())[:n_bars], tick_label=list(keyword_count.keys())[:n_bars])\nplt.xlabel(\"Word\")\nplt.ylabel(\"Number of Times Cited\")\nplt.title(\"Number of Times Each Work Was Cited\")\nplt.savefig(\"Category Commonalities\")\nplt.show()\n\n# If greater than like 15, kill them. Also, maybe categoies should be shrunk to like 15, a lot more readale this way\n\n","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.411927Z","iopub.status.idle":"2022-03-09T15:14:26.412515Z","shell.execute_reply.started":"2022-03-09T15:14:26.412294Z","shell.execute_reply":"2022-03-09T15:14:26.412318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Plotting (Where things become nice and pretty!)","metadata":{}},{"cell_type":"code","source":"! wget https://raw.githubusercontent.com/MaksimEkin/COVID19-Literature-Clustering/master/lib/plot_text.py\n! wget https://raw.githubusercontent.com/MaksimEkin/COVID19-Literature-Clustering/master/lib/call_backs.py\n! mkdir library\n! mv plot_text.py library/.\n! mv call_backs.py library/.\n! touch library/__init__.py\n! ls library/","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.413629Z","iopub.status.idle":"2022-03-09T15:14:26.414234Z","shell.execute_reply.started":"2022-03-09T15:14:26.413998Z","shell.execute_reply":"2022-03-09T15:14:26.414022Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# required libraries for plot\nfrom library.plot_text import header, description, description2, cite, description_search, description_slider, notes, dataset_description, toolbox_header \nfrom library.call_backs import input_callback, selected_code\nimport bokeh\nfrom bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper, CustomJS, Slider, TapTool, TextInput\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap, transform\nfrom bokeh.io import output_file, show, output_notebook\nfrom bokeh.plotting import figure\nfrom bokeh.models import RadioButtonGroup, TextInput, Div, Paragraph\nfrom bokeh.layouts import column, widgetbox, row, layout\nfrom bokeh.layouts import column","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.41544Z","iopub.status.idle":"2022-03-09T15:14:26.416017Z","shell.execute_reply.started":"2022-03-09T15:14:26.415795Z","shell.execute_reply":"2022-03-09T15:14:26.415819Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\n\ntopic_path = os.path.join(os.getcwd(), 'topics.txt')\nwith open(topic_path) as f:\n    topics = f.readlines()","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.417148Z","iopub.status.idle":"2022-03-09T15:14:26.417738Z","shell.execute_reply.started":"2022-03-09T15:14:26.417501Z","shell.execute_reply":"2022-03-09T15:14:26.417524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# show on notebook\noutput_notebook()\n# target labels\ny_labels = data['cluster']\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= X_embedded[:,0], \n    y= X_embedded[:,1],\n    x_backup = X_embedded[:,0],\n    y_backup = X_embedded[:,1],\n    desc= y_labels, \n    titles= data['title'],\n    authors = data['authors'],\n    journal = data['journal'],\n    abstract = data['abstract'],\n    labels = [\"C-\" + str(x) for x in y_labels],\n    links = data['doi'],\n    impact = data['score'].apply(lambda r: round(r, 2))\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles{safe}\"),\n    (\"Author(s)\", \"@authors{safe}\"),\n    (\"Journal\", \"@journal\"),\n    (\"Abstract\", \"@abstract{safe}\"),\n    (\"Link\", \"@links\")\n    (\"Impact\", \"@impact\")\n],\npoint_policy=\"follow_mouse\")\n\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[20],\n                     low=min(y_labels) ,high=max(y_labels))\n\n# prepare the figure\nplot = figure(plot_width=1200, plot_height=850, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset', 'save', 'tap'], \n           title=\"Clustering of the COVID-19 Literature with t-SNE and K-Means\", \n           toolbar_location=\"above\")\n\n# plot settings\nplot.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\",\n          legend = 'labels')\nplot.legend.background_fill_alpha = 0.6","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.418901Z","iopub.status.idle":"2022-03-09T15:14:26.419493Z","shell.execute_reply.started":"2022-03-09T15:14:26.419268Z","shell.execute_reply":"2022-03-09T15:14:26.419291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Keywords\ntext_banner = Paragraph(text= 'Keywords: Slide to specific cluster to see the keywords.', height=25)\ninput_callback_1 = input_callback(plot, source, text_banner, topics)\n\n# currently selected article\ndiv_curr = Div(text=\"\"\"Click on a plot to see the link to the article.\"\"\",height=150)\ncallback_selected = CustomJS(args=dict(source=source, current_selection=div_curr), code=selected_code())\ntaptool = plot.select(type=TapTool)\ntaptool.callback = callback_selected\n\n# WIDGETS\nslider = Slider(start=0, end=20, value=20, step=1, title=\"Cluster #\")\nkeyword = TextInput(title=\"Search:\", callback=input_callback_1)\n\n# pass call back arguments\ninput_callback_1.args[\"text\"] = keyword\ninput_callback_1.args[\"slider\"] = slider\n# column(,,widgetbox(keyword),,widgetbox(slider),, notes, cite, cite2, cite3), plot","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.420635Z","iopub.status.idle":"2022-03-09T15:14:26.421242Z","shell.execute_reply.started":"2022-03-09T15:14:26.421001Z","shell.execute_reply":"2022-03-09T15:14:26.421025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# STYLE\nheader.sizing_mode = \"stretch_width\"\nheader.style={'color': '#2e484c', 'font-family': 'Julius Sans One, sans-serif;'}\nheader.margin=5\n\ndescription.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndescription.sizing_mode = \"stretch_width\"\ndescription.margin = 5\n\ndescription2.sizing_mode = \"stretch_width\"\ndescription2.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndescription2.margin=10\n\ndescription_slider.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndescription_slider.sizing_mode = \"stretch_width\"\n\ndescription_search.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndescription_search.sizing_mode = \"stretch_width\"\ndescription_search.margin = 5\n\nslider.sizing_mode = \"stretch_width\"\nslider.margin=15\n\nkeyword.sizing_mode = \"scale_both\"\nkeyword.margin=15\n\ndiv_curr.style={'color': '#BF0A30', 'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndiv_curr.sizing_mode = \"scale_both\"\ndiv_curr.margin = 20\n\ntext_banner.style={'color': '#0269A4', 'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ntext_banner.sizing_mode = \"stretch_width\"\ntext_banner.margin = 20\n\nplot.sizing_mode = \"scale_both\"\nplot.margin = 5\n\ndataset_description.sizing_mode = \"stretch_width\"\ndataset_description.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ndataset_description.margin=10\n\nnotes.sizing_mode = \"stretch_width\"\nnotes.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\nnotes.margin=10\n\ncite.sizing_mode = \"stretch_width\"\ncite.style ={'font-family': 'Helvetica Neue, Helvetica, Arial, sans-serif;', 'font-size': '1.1em'}\ncite.margin=10\n\nr = row(div_curr,text_banner)\nr.sizing_mode = \"stretch_width\"","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.422435Z","iopub.status.idle":"2022-03-09T15:14:26.423024Z","shell.execute_reply.started":"2022-03-09T15:14:26.4228Z","shell.execute_reply":"2022-03-09T15:14:26.422824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# LAYOUT OF THE PAGE\nl = layout([\n    [header],\n    [description],\n    [description_slider, description_search],\n    [slider, keyword],\n    [text_banner],\n    [div_curr],\n    [plot],\n    [description2, dataset_description, notes, cite],\n])\nl.sizing_mode = \"scale_both\"\n\n\n# show\noutput_file('t-sne_covid-19_interactive.html')\nshow(l)","metadata":{"execution":{"iopub.status.busy":"2022-03-09T15:14:26.424157Z","iopub.status.idle":"2022-03-09T15:14:26.424738Z","shell.execute_reply.started":"2022-03-09T15:14:26.424501Z","shell.execute_reply":"2022-03-09T15:14:26.424524Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Clustering the Bottom 99 Percent of Data","metadata":{}},{"cell_type":"markdown","source":"## Conclusion\n    All in all, it was found that there was high correlation between each clusters, as well as a good corelation between impact and cluster size.","metadata":{}}]}